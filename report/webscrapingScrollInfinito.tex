\subsubsection{Infinite scroll websites}
Scrolling was the type of interaction required with client in both sites mentioned above: an "infinite" page kept loading contents progressively while being scrolled. This led to various issues that have been faced and solved in different ways. \\
The only solution found on the internet with a brief research on different sites wasn't very smart w.r.t. used physical resources \cite{currentscrollsolution}: these simple scripts just run an infinite loop in which the driver executes some lines of javascript to scroll the page down. Although this could fit good for some application/sites that doesn't require nor make possible (due to the presence of shrunk content on the page) to execute many times the scroll, if many years of financial articles needs to be loaded it will lead to a waste of memory and a sensible slow down of the application, e.g.:  after a year of news scraped in \url{https://www.nytimes.com/section/business/dealbook}, the Spider decelerated to the point where the javascript code was executed every 5 minutes, while it takes near to 1 seconds in the beginning. This happens because all the page is kept in memory (with many images displayed) and the DOM gets bigger at every request(?). Always referring to the previous example, after 1 year of news scraped, Firefox was using around 1.5 Gigabytes of RAM. \\
The first way tried to bypass the problem, was using the browser in headless mode, that is without graphic interface, (to do this, an option of Selenium driver needs to be set up) but the issue persisted. \\
The second, and working, solution is to dynamically remove HTML code from the page while it's being scraped. This has been done in different ways that are going to be compared later in the paper. 
\par 
% nytimes.com solution described here
\par 
Let's analyze the solution adopted for \url{https://www.marketwatch.com/newsviewer}: the content scraped with this Spider is the one displayed in the sidebar at the top of the page indexed by the link given. As for the foregoing case of nytimes.com, at first the unnecessary static html code of the page is removed executing some javascript lines. After that, a while true loop is entered, in which the driver execute the code necessary for the scroll: every 10 iterations, the sensitive data is retrieved and the various Item objects that weren't analyzed before, are yielded to the Pipeline; furthermore a script that removes each <li> tag is executed: in this way, the usage of physical resources is constant w.r.t the volume of information scraped. \\
The number 10 has been chosen empirically; indeed in this solution there were many problems related with the deletion of html contents and how ajax functions calls restore them and use them to generate the older articles. The revival of deleted news is handled by saving the timestamp of the latest article saved on physical support: the only news that will be written will be the ones previous to that timestamp.
\par 
% marketwatch.com second solution described here
\par  
% Insert techniques comparition here