\subsubsection{Infinite scroll websites}
Scrolling was the type of interaction required with client in both sites mentioned above: an "infinite" page kept loading contents progressively while being scrolled. This led to various issues that have been faced and solved in different ways. \\
The only solution found on the internet with a brief research on different sites wasn't very smart w.r.t. used physical resources \cite{currentscrollsolution}: these simple scripts just run an infinite loop in which the driver executes some lines of javascript to scroll the page down. Although this could fit good for some application/sites that doesn't require nor make possible (due to the presence of shrunk content on the page) to execute many times the scroll, if many years of financial articles needs to be loaded it will lead to a waste of memory and a sensible slow down of the application, e.g.:  after a year of news scraped in \url{https://www.nytimes.com/section/business/dealbook}, the Spider decelerated to the point where the javascript code was executed every 5 minutes, while it takes near to 1 seconds in the beginning. This happens because all the page is kept in memory (with many images displayed) and the DOM gets bigger at every request. Always referring to the previous example, after 1 year of news scraped, Firefox was using around 1.5 Gigabytes of RAM. \\
The first way tried to bypass the problem, was using the browser in headless mode, that is without graphic interface, (to do this, an option of Selenium driver needs to be set up) but the issue persisted. \\
The second, and working, solution is to dynamically remove HTML code from the page while it's being scraped. This has been done in different ways that are going to be compared later in the paper. 
\par 
The first site which implements infinite scrolling studied is \url{https://www.nytimes.com/section/business/dealbook}. The solution adopted for scraping nytimes is based on opening the site in device mode: in this way, instead of scrolling, a "Show more" button is displayed. Indeed, if the site is opened in desktop mode the button "Show more" disappears after the first time it gets clicked and infinite scrolling knocks in, while in mobile mode it is persistent. Once the scraper sets up the correct way of opening the site, it removes the unnecessary static HTML code from the page and a while true loop starts. The algorithm at first clicks on "Show more", then it takes the new articles loaded by the server and at the end it yields them to the pipeline for saving them. Subsequently, the script for removing all the <li> tags is executed in order to reduce the physical resources used by the page. Finally, the button is clicked again and the cycle is repeated. However, when the script scrapes more than two years of financial news, it becomes a bit slow. 
\par 
The solution adopted for \url{https://www.marketwatch.com/newsviewer}: the content scraped with this Spider is the one displayed in the sidebar at the top of the page indexed by the link given. As for the foregoing case of nytimes.com, at first the unnecessary static html code of the page is removed by executing some javascript lines. After that, a while true loop is entered, in which the driver executes the code necessary for the scroll: every 10 iterations, the sensitive data is retrieved and the various Item objects that weren't analyzed before, are yielded to the Pipeline; furthermore a script that removes each <li> tag is executed: in this way, the usage of physical resources is constant w.r.t the volume of information scraped. \\
The number 10 has been chosen empirically: indeed, in this solution there were many problems related with the deletion of HTML contents and how ajax functions calls restore them and use them to generate the older articles. For instance, it's been tried to delete all the tags, or a part of them such as an half, in each iteration, but that wasn't working because they were re-established instantly, thus slowing down the whole process. Doing that each 10 iterations allowed to keep a good velocity because not the totality of the articles were spawn; however, the revival of deleted news still needs to be handled: the timestamp of the latest article stored on physical support is saved: the only news that will be written are the ones previous to that timestamp.
\par 
Despite having already scraped the site marketwatch.com, there is another solution adopted in this project: the content scraped in this case is in the sidebar at the bottom of the page. The technique used this time is somewhat called javascript injection and the process is composed of two parts:
\begin{itemize}
	\item First of all, it's necessary to analyze the code written by the site developer to find the code that send a request to the server to get the oldest news. The javascript code is accessible by browser using "View Source".
	\item After finding the code and injecting the javascript executable command, it is possible to get the old news.
\end{itemize}
This method avoid many of the problems about the infinite scrolling and can also exploit the normal update of the news (e.g. asking the server to get a ton of news instead of a fixed number determined by the site developer constraint) but it's not ideal. 
In this solution, while analyzing the javascript code, it has been detected a fix for the reloading of the news; Basically this site every 60 seconds load all the newer news only if the height of the sidebar page is at the top, but for the objective of scraping old news, these news are trivial. So the trick or solution adopted , when deleting the news already scraped, is to leave a good amount of news and scroll the sidebar page at the half of its maximum height, so the javascript written by MarketWatch doesn't work.
\par
The study of the code written by someone else is quite a difficult task to do, because it depends on the skill of the counterpart programmer and it's not always possible to find the code that calls the server. At the end this kind of technique is quite trivial but more efficient compared to two previous methods.
\par
% Insert techniques comparition here