\subsubsection{Infinite scroll websites}
Scrolling was the type of interaction required with client in both websites mentioned above: an "infinite" page kept loading contents progressively while being scrolled. This led to various issues that have been faced and solved in different ways. \\
The only solution found on the internet with a brief research on different websites wasn't very smart w.r.t. used physical resources \cite{currentscrollsolution}: these simple scripts just run an infinite loop in which the driver executes some lines of javascript to scroll the page down. Although this could fit good for some application/websites that doesn't require nor make possible (due to the presence of shrunk content on the page) to execute many times the scroll, if many years of financial articles needs to be loaded it will lead to a waste of memory and a sensible slow down of the application, e.g.:  after a year of news scraped for \url{https://www.nytimes.com/section/business/dealbook}, the Spider decelerated to the point where the javascript code was executed once every 5 minutes, while it took nearly 1 second in the beginning. This happens because the whole page is kept in memory (with many images displayed) and the DOM gets bigger at every request. Always referring to the previous example, after 1 year of news scraped, Firefox was using around 1.5 GB of RAM. \\
The first way we tried to bypass the problem, was using the browser in headless mode, that is without graphic interface, (to do this, an option of Selenium driver needs to be set up) but the issue persisted. \\
The second, and working, solution was to dynamically remove HTML code from the page while it's being scraped (i.e. by executing javascript removing scripts with Selenium). This has been done in different ways that are going to be compared later in this report. 
\par 
The first website which implements infinite scrolling is \url{https://www.nytimes.com/section/business/dealbook}. The solution adopted for scraping the nytimes is based on opening the website in device-mode (i.e. with browser in mobile-mode): in this way, instead of scrolling, a "Show more" button is displayed. In fact, if the website is opened in desktop mode the button "Show more" disappears after the first time it is clicked and the infinite scrolling knocks in, while in mobile mode it is persistent. Once the scraper sets up the correct way of opening the website, it removes the unnecessary static HTML code from the page and a while true loop starts. The algorithm, at first, clicks on "Show more", then it takes the new articles loaded and it provides them to the pipeline for saving them. Subsequently, the script for removing all the <li> tags is executed in order to reduce the physical resources used by the page. Finally, the button is clicked again and the cycle is repeated. However, when the script scrapes more than two years of financial news, it becomes a bit slow. 
\par 
The solution adopted for \url{https://www.marketwatch.com/newsviewer} works as follow: the content scraped with this Spider is the one displayed in the sidebar at the top of the page indexed by the link given. As for the case of nytimes.com, at first the unnecessary static HTML code of the page is removed by executing some javascript lines. After that, a while true loop is entered, in which the driver executes the code necessary for the scroll: every 10 iterations, the interesting data is retrieved and the various Item objects that weren't analyzed before are provided to the Pipeline; furthermore, a script that removes each <li> tag is executed: in this way, the usage of physical resources is constant w.r.t the volume of information scraped. This happens because the rise of quantity of <li> tags and the dimension of the DOM are programmatically limited by the algorithm, which removes data that has already been scraped.\\
The number 10 has been chosen empirically: indeed, in this solution there were many problems related with the deletion of HTML contents and how the ajax functions calls restore them and use them to generate the older articles. For instance, it's been tried to delete all the tags, or a part of them such as an half, in each iteration, but that wasn't working because they were re-established instantly by the ajax functions, thus slowing down the whole process: more specifically, the majority of the contents were restored, thus leading to analyze the same contents many times. Doing that each 10 iterations allowed to keep a good velocity because not the totality of the articles were spawn; however, the revival of deleted news still needs to be handled: the timestamp of the latest article stored on physical support is saved: the only news that will be written are the ones previous to that timestamp. This is necessary because the ajax calls restore <li> tags that analyzed in the previous iteration. 
\par 
Despite having already found a feasible solution for the marketwatch.com website, we identified another  another solution: the content scraped in this case is in the sidebar at the bottom of the page. The technique used this time is called "javascript injection" and the process is composed of two parts:
\begin{itemize}
	\item First of all, it's necessary to analyze the code written by the website developer to find the code that sends a request to the server to get the oldest news. The javascript code is accessible by the browser using "View Source".
	\item After finding the code and injecting the javascript executable command, it is possible to get the old news.
\end{itemize}
This method avoids many of the problems about the infinite scrolling and can also exploit the natural update of the news (e.g. asking the server to get a chosen number of news instead of a fixed one determined by the website developer constraint) but it's not ideal. 
In this solution, while analyzing the javascript code, it has been detected a fix for the "reloading" of the news; Basically, this website load newer news every 60 seconds only if the height of the sidebar page (this container of news has a scrollbar) is at the top (i.e. it means that the scroll position is at the top), but for the objective of scraping old news, these newer news are trivial. So the trick adopted is to set the position of the scrollbar at half from top and bottom, so the javascript written by MarketWatch developers doesn't run.
\par
The study of the code written by someone else is quite a difficult task to do, because it depends on the skill of the counterpart programmer and it's not always possible to find the code that calls the server. All in all, despite this technique being quite trivial, revealed to be efficient w.r.t. the two previous methods.
\par
% Insert techniques comparition here