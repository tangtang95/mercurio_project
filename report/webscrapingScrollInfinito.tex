\subsubsection{Infinite scroll websites}
Scrolling was the type of interaction required with client in both sites mentioned above: an "infinite" page kept loading contents progressively while being scrolled. This led to various issues that have been faced and solved in different ways. \\
The only solution found on the internet with a brief research on different sites wasn't very smart w.r.t. used physical resources \cite{currentscrollsolution}: these simple scripts just run an infinite loop in which the driver executes some lines of javascript to scroll the page down. Although this could fit good for some application/sites that doesn't require nor make possible (due to the presence of shrunk content on the page) to execute many times the scroll, if many years of financial articles needs to be loaded it will lead to a waste of memory and a sensible slow down of the application, e.g.:  after a year of news scraped in \url{https://www.nytimes.com/section/business/dealbook}, the Spider decelerated to the point where the javascript code was executed every 5 minutes, while it takes near to 1 seconds in the beginning. This happens because all the page is kept in memory (with many images displayed) and the DOM gets bigger at every request(?). Always referring to the previous example, after 1 year of news scraped, Firefox was using around 1.5 Gigabytes of RAM. \\
The first way tried to bypass the problem, was using the browser in headless mode, that is without graphic interface, (to do this, an option of Selenium driver needs to be set up) but the issue persisted. \\
The second, and working, solution is to dynamically remove HTML code from the page while it's being scraped. This has been done in different ways that are going to be compared later in the paper. 
\par 
The first site which implements infinite scrolling studied is \url{https://www.nytimes.com/section/business/dealbook}. The solution adopted for scraping nytimes is based on opening the site in device mode: in this way, instead of scrolling, a "Show more" button is displayed. Indeed, if the site is opened in desktop mode the button "Show more" disappears after the first time it gets clicked and infinite scrolling knocks in, while in mobile mode it is persistent. Once that the scraper sets up the correct way of opening the site, it removes the unnecessary static HTML code from the page and a while true loop starts. The loop at first clicks on "Show more", it takes the new articles loaded by the server and it yields them to the pipeline for saving them. Subsequently, the script for removing all the <li> tags is executed in order to reduce the physical resources used by the page. Finally, the button get clicked again and the cycle is repeated. However, when the script scrapes more then two years of financial news it becomes a bit slow. 
\par 
The solution adopted for \url{https://www.marketwatch.com/newsviewer}: the content scraped with this Spider is the one displayed in the sidebar at the top of the page indexed by the link given. As for the foregoing case of nytimes.com, at first the unnecessary static html code of the page is removed executing some javascript lines. After that, a while true loop is entered, in which the driver executes the code necessary for the scroll: every 10 iterations, the sensitive data is retrieved and the various Item objects that weren't analyzed before, are yielded to the Pipeline; furthermore a script that removes each <li> tag is executed: in this way, the usage of physical resources is constant w.r.t the volume of information scraped. \\
The number 10 has been chosen empirically: indeed, in this solution there were many problems related with the deletion of HTML contents and how ajax functions calls restore them and use them to generate the older articles. For instance, it's been tried to delete all the tags, or a part of them such as an half, in each iteration, but that wasn't working because they were re-established instantly, thus slowing down the whole process. Doing that each 10 iterations allowed to keep a good velocity because not the totality of the articles were spawn; however, the revival of deleted news still needs to be handled: the timestamp of the latest article stored on physical support is saved: the only news that will be written are the ones previous to that timestamp.
\par 
Despite having already scraped the site of MarketWatch, there is another solution adopted in this project: the content scraped in this case is the bottom newsviewer of the same page as before. The technique used this time is somewhat called javascript injection:
\begin{itemize}
	\item It consists on injecting javascript executable command based on the written code inside the site.
	\item 
\end{itemize}
% marketwatch.com second solution described here
\par  
% Insert techniques comparition here