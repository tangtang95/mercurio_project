\section{Web scraping}
Web scraping is extracting structured data from websites. The term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying, in which specific data is gathered and copied from the web, generally into a central local database or spreadsheet, for later retrieval or analysis \cite{webscrapingwiki}. \\
The first target that had to be accomplished to turn the Mercurio project and its analysis to an international version was the data collection of financial news from various sources using web scraping techniques. In particular, the websites that have been considered were: 
\begin{itemize}
\item \href{https://www.bloomberg.com}{bloomberg.com}
\item \href{https://www.nytimes.com}{nytimes.com}
\item \href{https://www.thisismoney.co.uk}{thisismoney.co.uk}
\item \href{http://money.cnn.com}{money.cnn.com}
\item \href{http://www.marketwatch.com}{marketwatch.com}
\item \href{http://www.reuters.com}{reuters.com}
%\item \href{http://www.investing.com}{investing.com}
\item \href{http://www.moneymorning.com}{moneymorning.com} 
\item \href{http://www.4-traders.com/}{4-traders.com}
\end{itemize}
An important distinction among the sources mentioned above is whether or not is required an ajax/javascript interaction with the user in the web page that arranges the articles under consideration. Those that do not involve these interaction will be treated in the "static websites" section, the leftovers in the "dynamic websites" section, with a focus on infinite scroll websites. \\ 
To manage this issue of the project, the Scrapy open source framework \cite{scrapyframework} has been used. 
\par
The creation of the project has been done with the command: "scrapy startproject tutorial" from command line interface; this will create the template composed with a series of folder for the implementation of spiders.
Once the project has been created, a single Spider \cite{scrapyspider} for each source has to be set up: they are the classes that define how a source will be scraped, and in particular, how to perform the crawling (i.e. follow links) and how to extract structured data from the page. The Spider's lifecycle varies a little between dynamic and static websites, but basically it starts by sending a request to URLs specified in starting\_url, afterwards it gets the data, stores it on some physical support (e.g. database, files) and finally crawls to another source or invokes some javascript commands and repeats. The single steps could be more or less complicated depending on the considered source. \\
In order to make the storing stage work, the Scrapy Item Pipeline \cite{scrapypipeline} has been used. It does a very simple task, once it receives an item that changes according to the source, it performs some action over it (i.e. store it).\\
Depending on the type of information that the website provides, the items that can be stored using the pipeline are two: "BriefItem", that contains a field for title, date, time, and an eventual URL where the news is better specified, and "NewsItem", which carries a value for title, authors, date, time, content and keywords. The former has been used when the use of the latter didn't make sense, due to a lack of information on the website, or due to the impossibility of retrieving it: for example, some websites (e.g. Marketwatch) provide a list of links to financial articles from many different websites, making out of the question to get the more specified data from such different website formats. The "date" saved for each news is synchronized on the UTC timezone by using the "pytz" library.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Architettura}
\caption{Architettura del framework di Scrapy}\label{fig:1}
\end{figure}

\subsection{Saving the data}
When it comes to saving the data there is a multitude of possibilities, ranging from a simple text file to a more structured database. 
At first, for testing purpose, the data has been saved in a tsv(i.e. tab separated values) textual file. Subsequently, in the deployment stage, it has been memorized on a database. For the management of the code it has been chosen to keep both the database and the files using a strategy pattern. This means that in order to test new scraping scripts in the future, it is possible to switch to the file methodology changing only few lines of code. 
\par
As it is shown on the architecture, the item(i.e. BriefItem or NewsItem) goes through a pipeline, but before that, it must be loaded with data.
The initialization of an item can be done with two methods:
\begin{itemize}
	\item Creating an item with its constructor;
	\item Using an item loader to fill the field of the item.\\
	This kind of object lets the developer define two preprocessing operations for each field of the item:
	\begin{itemize}
		\item input preprocessing, which provides a set of operations to be done when the data is filled inside the item loader (e.g. remove tags, remove escape characters, etc...)
		\item output preprocessing, which provides an operation to be done when the item is created (e.g. taking only the first data of that specific field if there are a lot of data or concatenating those strings gotten through scraping)
	\end{itemize}
\end{itemize}

\par While structuring and developing the scripts, no design pattern has been identified and used, except for the structure that the Scrapy framework itself already has. In fact the single steps that have to be performed have strong dependencies with  the websites' structure and the ways that they arrange the information. For instance, most of the scripts are really light in term of lines of code, and the majority of them regards mainly the way in which the websites arranges its information and articles (e.g. a set of <li> tags). The only model that came to mind is the Template method pattern \cite{templatepattern}, but it only introduces redundancy with the Scrapy framework structure. 
\input{webscrapingStatico}
\input{webscrapingDinamico}
\input{scrapyDeploy}