\section{Web scraping}
The first target that had to be accomplished to turn the Mercurio project and its analysis to an international version was the data collection of financial news from various sources by web scraping techniques. In particular, the sites that had been considered were: 
\begin{itemize}
\item \href{https://www.bloomberg.com}{bloomberg.com}
\item \href{https://www.nytimes.com}{nytimes.com}
\item \href{https://www.thisismoney.co.uk}{thisismoney.co.uk}
\item \href{http://money.cnn.com}{money.cnn.com}
\item \href{http://www.marketwatch.com}{marketwatch.com}
\item \href{http://www.reuters.com}{reuters.com}
\item \href{http://www.investing.com}{investing.com}
\item \href{http://www.moneymorning.com}{moneymorning.com} 
\item \href{http://www.4-traders.com/}{4-traders.com}
\end{itemize}
An important distinction among the sources mentioned above is whether or not is required an ajax/javascript interaction with the user in the web page that arranges the intended articles. The ones that do  not involve these interaction will be treated in the "static sites" section, the leftovers beneath "dynamic sites" section, with a focus on infinite scroll websites. \\ 
To manage this side of the project, the Scrapy open source framework \cite{scrapyframework} had been used. % Insert project creation here, but before check README % \\
Once the project has been created, essentially, a single Spider \cite{scrapyspider} for each source has to be set up: they are the classes that define how a source will be scraped, and in particular, how to perform the crawl (i.e. follow links) and how to extract structured data from the page. The Spider's lifecycle varies a little between dynamic and static sites, but basically, it starts sending a request to links specified in starting\_url, afterwards it gets the data, writes it on some physical support (e.g. database, files) and finally crawls to another source or invokes some javascript commands and repeats. The single steps could be more or less complicated depending on the considered source. \\
In order to make writing stage work, the Scrapy Item Pipeline \cite{scrapypipeline} has been used. It does a very simple stuff, once it receives an item that changes according to the source, it performs some action over it (i.e. store it). Until this moment, the type of items that can be written are two: "BriefItem", that contains a field for title, date, time, and an eventual url where the news is better specified, and "NewsItem", which carries a value for title, authors, date, time, content and keywords. The former has been used, when the use of the latter didn't make sense, due to a lack of information on the sites, or due to the impossibility of retrieving it: for example, some sites (e.g. Marketwatch) provide a list of links to financial articles from many different websites, making out of the question to get the more specified data from such different site formats.   
% Insert strategy pattern for saving the data
% Insert item loader here% 
% Insert stuff regarding scroll pause time and robots.txt
% Insert stuff regarding the date here
\par While structuring and developing the software, none design pattern has been identified and used, except the structure that Scrapy framework itself has. In fact the single steps that have to be performed have strong dependencies with sites' structure and the ways that they arrange the information. (Inserire qui un esempio). The only model came to mind is Template method pattern \cite{templatepattern}, but it only introduces redundancy with Scrapy framework structure. 
\input{webscrapingStatico}
\input{webscrapingDinamico}
\input{scrapyDeploy}