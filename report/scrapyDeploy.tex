\subsection{Deployment}
After finishing the development of the spiders, it's necessary to deploy the project on a server. 
For this, the protocol SSH(Secure Shell)\cite{ssh} has been used to access the server and create the table of the data.
Since the server is accessed by more than one person, it has been decided to use the command 'screen' to create a new thread screen that keeps running on background even if the connection to the server is closed. For the database it has been used MySQL and a python library called MySQLdb; the queries used are:
\begin{minted}[mathescape,
               linenos,
               numbersep=5pt,
               gobble=0,
               frame=lines,
               framesep=2mm]{sql}
CREATE TABLE articles_en_full(
    id INT NOT NULL PRIMARY KEY AUTO_INCREMENT,
    date DATE,
    time TIME,
    title VARCHAR(100),
    newspaper VARCHAR(30),
    author VARCHAR(50),
	content TEXT,
    tags TEXT,
)

CREATE TABLE articles_en_partial(
    id INT NOT NULL PRIMARY KEY AUTO_INCREMENT,
    date DATE,
    time TIME,
    title VARCHAR(255),
    newspaper VARCHAR(100),
    url VARCHAR(200),
)
\end{minted} 
In the section of deploying crawlers, it's very important to make use of a server to avoid the executions on client machines: this is due to the fact that servers have more stability in terms of connection and computing power. 
This is why the scraping project needs to be deployed on a server; to solve this issue "scrapy" offers an application for deploying and running Scrapy spiders, called scrapyd\cite{scrapyd}. 
This daemon, background process, is executed by using screen and the command 'scrapyd' on a server waiting for messages on a specific port. 
Another feature offered by scrapyd is a set of JSON API to deploy the projects and scheduling the spiders; for example:
\begin{itemize}
	\item daemonstatus.json, which returns the status of the scrapyd application that is running on the server
	\item addversion.json to deploy a project
	\item schedule.json to schedule a spider
	\item cancel.json to stop a spider
\end{itemize}
One more way to deploy the project is to use an application client for scrapyd, called scrapyd-client\cite{scrapydclient}; but this doesn't provide every API operations that scrapyd offers natively. The solution adopted for Mercurio is a mix of both technique. 
For the deployment of the project it has been used  the command of scrapyd-client while for the canceling of the spider it has been used the JSON API cancel.json.
\par
Before the compilation of the server, it's important to define what package, folders and files are necessary for the project deployment and then keep going with the deployment; this is defined by the file setup.py auto generated by scrapyd. 
In this project, the scrapyd daemon is running on a server of the Politecnico of Milano listening on port 6800. 
In many cases, the server could be unreachable from the outside (e.g. internet) due to the presence of firewalls, so before deploying, it's necessary to make a port forwarding to the server via SSH protocol. 
After deploying the project it's possible to schedule spiders in order to crawl data and store it; to check the scheduled spiders and their logs, scrapyd offers a minimal GUI web interface accessible through browser at localhost:9000. \\
A server owned by Politecnico di Milano has been used to host scrapyd, used with the support of scrapyd-client, and to save the data on a MySQL database.