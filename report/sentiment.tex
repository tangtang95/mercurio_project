\subsection{Sentiment analysis}
The purpose of this analysis is to give each article a sentiment, in order to allow pattern mining to forecast financial catastrophes. This approach has been implemented in the english version of the project because of the well-trained nlp libraries available and due to the fact the international press was thought to be more vigorous and less apathetic compared to the italian one. \\
The sentiment tool provided by Stanford CoreNLP parses a sentence and returns "very negative", "negative", "neutral", "positive" or "very positive", depending on the deep learning model \cite{sentimentdeep} used in its implementation. So it's been necessary to find a way of weighting the results associated with each sentences presents in the article, paying attention to the fact that neutral values are relevant and not negligible \cite{neutralvalues}. Let's tackle the problem in two steps, each of which highlights different things.
\begin{itemize}
\item
First of all, clearly not all of the phrases in an article have the same level of importance, thus some periods should weight more than others. The solution adopted to overcome this problem is creating a lemmatized vocabulary composed of financial nouns and keywords. When parsing a sentence, the more it contains this terms, the more it weights, following a linear relation. \\
\begin{math}
weight = 1 + 0.5*n \\
\end{math}
Where n is the number of words present in the dictionary that appear in the sentence too.
The multiplier coefficient (0.5) has been chosen in an empirical way and can be subject of further research and learning models. \\
The weight is then used to modify a python dictionary that has as keys "very negative", "negative", "neutral", "positive" and "very positive" according to the result given by Stanford CoreNLP sentiment annotator.
\item 
When it comes to assigning the sentiment of the article, the dictionary mentioned above comes into play: the object is to determine a number between 1 and -1 that represents the sentiment of the article (w\textsubscript{a}). The interval [-1,+1] is then divided in five equals parts, each of which is assigned to a sentiment and according to w\textsubscript{a} the article's sentiment is calculated.\\
Let be w\textsubscript{vn}, w\textsubscript{n}, w\textsubscript{ne}, w\textsubscript{p}, w\textsubscript{vp} the weights of "very negative", "negative", "neutral, "positive" and "very positive" for the whole article respectively, and m\textsubscript{vn}, m\textsubscript{n}, m\textsubscript{ne}, m\textsubscript{p} and m\textsubscript{vp} their central position in the [-1,+1] interval. Being i the position of the sentiment mentioned above:\newline \newline
\begin{math}
i \in N, i \in [0,4] \\
\end{math}
\begin{equation}
m_i= -0.8+i*0.4
\end{equation}
\begin{equation}
w_a = \sum\limits_{i} w_i * m_i
\end{equation}
Doing so, some important properties are guaranteed: neutral values are relevant and w\textsubscript{a} gets balanced with equals numbers of negative and positive weights. However, this approach won't permit to compare different level of positiveness and negativeness. 
\end{itemize}
However, there is a problem with the whole algorithm, that regards linguistic itself. Let's consider the phrase "Facebook is near to declare bankrupt due to the recent scandal. Its competitors can take a big advantage from this.". The first period is valued "negative", while the latter "positive", so a further implementation has to consider this fact: the sentiment depends even on the subject of the period. For example, an open road could be developing the sentiment analysis for only a small group of companies, and implementing a subject recognizer for the article and the single sentences, in order to calculate in a more precise way w\textsubscript{a}. The focus on a small group of companies allows, for instance, to associate each firm with its CEO.  
\par
Conclusion : to write when executed on the server.