\subsection{Sentiment analysis}
Sentiment analysis refers to the use of natural language processing and text analysis to detect automatically, extract, quantify and study states and subjective information. It aims to determine the attitude of a writer/speaker w.r.t some topic or the overall contextual polarity or emotional reaction to a document, interaction, or event \cite{sentimentwiki}. \\
The purpose of our analysis is to give each article a sentiment, in order to allow pattern mining to forecast financial catastrophes. This approach has been implemented in the english version of the project because of the well-trained nlp libraries available and due to the fact the international press was thought to be more vigorous and less apathetic compared to the italian one. \\
The sentiment tool provided by Stanford CoreNLP parses a sentence and returns "very negative", "negative", "neutral", "positive" or "very positive", depending on the deep learning model \cite{sentimentdeep} used in its implementation. So it's been necessary to find a way of weighting the results associated with each sentence found in the article. \\
The problem has been approached in three different ways that will be explained in the following pages.
However, every implemented solution is based on the fact that clearly not all of the phrases in an article have the same level of importance, thus some periods should weight more than others. The solution adopted to overcome this problem is creating a lemmatized vocabulary composed of financial nouns and keywords. When parsing a sentence, the more it contains these terms, the more it weights, following a linear relation. \\
\begin{math}
weight = 1 + 0.5*n \\
\end{math}
Where n is the number of words present in the dictionary that appear in the sentence too.
The multiplier coefficient (0.5) has been chosen in an empirical way and can be subject to further research and learning models. \\
The weight is then used to modify a python dictionary which has as keys "very negative", "negative", "neutral", "positive" and "very positive" according to the result given by Stanford CoreNLP sentiment annotator. 

\subsubsection{Sentiment analysis with evaluation on neutral values}
Due to the fact that studies have reported the importance and relevance of neutral values \cite{neutralvalues}, in this first method they are considered not negligible. Furthermore, the whole text of the financial news is considered.\\
So, once that the algorithm described above is applied, when it comes to assigning the sentiment of the article, the dictionary comes into play: the aim is to determine a number between 1 and -1 that represents the sentiment of the article (w\textsubscript{a}). The interval [-1,+1] is then divided in five equals parts, each of which is assigned to a sentiment and according to w\textsubscript{a} the article's sentiment is calculated.\\
Let be w\textsubscript{vn}, w\textsubscript{n}, w\textsubscript{ne}, w\textsubscript{p}, w\textsubscript{vp} the weights of "very negative", "negative", "neutral, "positive" and "very positive" for the whole article respectively, and m\textsubscript{vn}, m\textsubscript{n}, m\textsubscript{ne}, m\textsubscript{p} and m\textsubscript{vp} their central position in the [-1,+1] interval. Being i the position of the sentiment mentioned above:\newline \newline
\begin{math}
i \in N, i \in [0,4] \\
\end{math}
\begin{equation}
m_i= -0.8+i*0.4
\end{equation}
\begin{equation}
w_a = \frac{\sum\limits_{i} w_i * m_i}{\sum\limits_{i} w_i}
\end{equation}
Doing so, some important properties are guaranteed: neutral values are relevant and w\textsubscript{a} gets closer to the most valued voice of the dictionary. An important note is that this approach won't permit to compare different level of positiveness and negativeness. \\
Nevertheless, once that the script's been run on the server, the results didn't match the expectations: the majority, or, for better saying, almost the totality, of the articles were valued as "neutral" due to the fact that a huge number of phrases contained were analyzed "neutral" as well from NLP. This is the main reason that lead to the development of the second and the third methods.

\subsubsection{Sentiment analysis neglecting neutral values}

The second method that has been studied neglects the presence of neutral values. The only thing that is going to change w.r.t. the precedent solution, is 
\begin{equation}
w_a = \frac{\sum\limits_{i} w_i * m_i}{\sum\limits_{i} w_i} 
\end{equation}
In this case in the denominator the summation doesn't involve w\textsubscript{ne}. In this way, the huge amount of weight of neutral sentences is nullified, thus making the algorithm much more susceptible to the presence of positive or neutral phrases.\\

\subsubsection{Sentiment analysis of summarized text}
The last solution makes use of a summarization script, which was developed for the italian version of Mercurio and that has been adjusted for the english version by modifying the list of stop words. The mentioned summarization script makes use of a combination of extractive and abstractive summarization methods. The former consists in selecting important phrases and concatenating them into a shorter form by means of statistical and linguistic features, while the latter adopts linguistic methods to examine and interpret the text in order to understand the main concepts in a document. The result contains the 25\% of the original sentences. \\
Due to the fact that a summary contains the most important sentences of an article, analyzing the abstract of a financial news by removing the most of the content could benefit the result because, as already said, not all of the phrases present in it weights the same. \\
The summarization has been combined with the strategy described in the first method. 

\subsubsection{Considerations on sentiment analysis}
By querying the database it is possible to infer some conclusions. The efforts for making the algorithm more susceptible to positive and negative phrases were totally useless: all the row of the three columns saved in the database reports only "neutral" result. This is due to the fact that the initial hypothesis that american press isn't apathetic is basically false. In fact, the script has been launched printing the sentiment of each analyzed sentence, and they are all reported as "neutral". This is clearly a problem for the entire process. \\
Furthermore, independently on the method considered, there is a problem with the whole algorithm that regards linguistic itself. Let's consider the phrase "Facebook is near to declare bankrupt due to the recent scandal. Its competitors can take a big advantage from this.". The first period is valued "negative", while the latter "positive", so, a further implementation has to consider this fact: the sentiment depends even on the subject of the period: some fact could carry a benefit to a company at the expanse of its competitor, for instance. A future work could be developing the sentiment analysis for only a small group of companies, and implementing an entity recognizer for the articles and the single sentences, in order to calculate in a more precise way w\textsubscript{a}. The focus on a small group of companies allows, furthermore, to associate each firm with its CEO, in a deterministic way.  