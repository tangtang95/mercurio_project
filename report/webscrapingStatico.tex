\subsection{Static sites}
Nytimes.com and marketwatch.com are the only sources considered that doesn't fall under this category: every other sites doesn't require any kind of interaction with the client. In particular, the remaining ones can be further split in the ones that needs to scraped by modifying a value in the URL (i.e. the page number of a certain section) and the ones for which their sitemap is used and analyzed to retrieve the links of the interested articles. 
\par
Investing.com and 4-traders.com are the sites that fall under the first of the two classes illustrated above. The parse method starts from a certain page number (zero) and retrieves the information require; after that, if necessary, another function that scrape the single articles is called (this happens in 4-traders.com). Finally, a request to the next page is done and the parse method will handle it.
\par 
The easiest way to scrape data from a website is via sitemap, but not every sites has. 
The sitemap is a special file XML containing a list of every "core" information of the website like news, videos, stocks and so on; it is important for the web company itself, because huge crawler like googlebot, yahoobot and other company's bot could use this to search for data and at the end the process can increase its google rank (google also ranks web pages and not just websites). 
Of course, there are other benefits like:
\begin{itemize}
	\item facilitate the work of the crawler
	\item simplify the categorization of content
	\item reduce the amount of jobs to monitor the visitors
\end{itemize}
Another essential thing to look for is the file "robots.txt", which every sites has, which contains all the rules a crawler should follow and sometimes there is also a sitemap. 
This is why before scraping everyone should examine this particular file. 
\par 
This project, with the help of Scrapy, has searched through a lot of websites (Bloomberg, CNN, This Money,...) via sitemaps using an XML parser offered by Scrapy. 
Unlike the dynamic sites these are simpler to implement and that's why there are no particular ploy to use to increase the efficiency of the crawler. 
Although, there is a big problem when someone scrape data with nonchalance. 
The web company sometimes doesn't want every user agent to crawl its website, so they take countermeasures like temporary ban. 
For example Bloomberg has implemented a system detection to identify unauthorized crawlers with the purpose of banning them; to solve partially this case, it's possible to implement a rotation of ip addresses synchronized with a rotation of user agents. 
This method can maybe avoid the system detection, but for Bloomberg, it just increase the number of GET request before getting banned. 
Nevertheless the sitemap approach is still one of the best technique to scrape data.